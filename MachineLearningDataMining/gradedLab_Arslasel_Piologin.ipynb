{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Data Mining: Graded Lab 01\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grading\n",
    "For this graded lab you can get a total of 15 points. These 15 points count 10% of your final grade for the course.\n",
    "\n",
    "#### Deadline\n",
    "You have 20 days to finish the lab. You must submit the graded lab the evening before the lab-session. \n",
    "\n",
    "- **If you have lab on Friday: Hand-in on 11.11.21, 11.59 p.m.**\n",
    "- **If you have lab on Saturday: Hand-in on 12.11.21, 11.59 p.m.**\n",
    "\n",
    "#### Submission\n",
    "You have to solve the labs in groups of **two** students. If the class has an uneven number of students, there can be one group of three students. For this group, however, task 3 is graded more strictly and extra effort is expected.\n",
    "\n",
    "Submit your GradedLab01.pynb file renamed to **LastnameStudent01_LastnameStudent02.ipynb** in moodle.   \n",
    "Please submit a runnable python notebook file **and** an exported HTML with all cells executed (the results must be visible in the HTML).\n",
    "All other submissions will be rejected and graded with 0 points.\n",
    "\n",
    "#### Presentation Exercise 3\n",
    "The students are completely free to decide how to solve exercise 3. If the exercise is not solved in this notebook, a detailed description with screenshots must be included. In any case, task 3 must be presented to the lecturer during class. Thereby, a live demonstration is expected and not a presentation with PowerPoint. This demonstration should take about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wichtiges:\n",
    "#-lauffähiges Python notebook und mit exportierter HTMLversion -> Sonst Note1\n",
    "#-Projektdatei muss so heissen: PioLogin01_Arslan02.ipynb\n",
    "#-Aufgabe3 muss präsentiert werden, ca. 5min live oder über MS-Team. Kein Powerpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement a content-based recommender for the data in the movies.csv file [4 points].    \n",
    "The movies.csv file contains a dataset extracted from IMDB with several attributes. For example, the title, plot, and the language of the movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __(a) The user Calvin hasn't watched any movies yet. Print the top 5 movies based on the rating attribute, as recommendations for him [0.5 points].__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df = pd.read_csv('movies.csv')\n",
    "pd.set_option('max_colwidth', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSortedByRating = df.sort_values(by=['rating'], ascending=False)\n",
    "dfTop = dfSortedByRating.head(5)\n",
    "dfTop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __(b) The user Calvin hasn't still watched any movies yet. We  just know that he likes english speaking movies. Print the top 5 movies based on the rating attribute, which have only the language \"English\" in the languages attribute, as recommendations for him [0.5 points].__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFiltered = df[df['languages'] == '[\\'English\\']']\n",
    "dfFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSorted = dfFiltered.sort_values(by=['rating'], ascending=False)\n",
    "dfTopEnglish = dfSorted.head(5)\n",
    "dfTopEnglish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __(c) Still Calvin hasn't still watched any movies yet. However, we noticed that he took more time on the recommendations with the genres \"Drama\" and \"Crime\", therefore we assume that he is interested in this genre combination. Print the top 3 movies based on the rating attribute, which have at least the genres \"Drama\" and \"Crime\" as recommendations for him [0.5 points].__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSortedCrimeDrama = df[(df['languages'] == '[\\'English\\']') & (df['genres'].str.contains('Drama' and 'Crime'))]\n",
    "dfSortedCrimeDrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSortedCD = dfSortedCrimeDrama.sort_values(by=['rating'], ascending=False)\n",
    "dfTopCD = dfSortedCD.head(3)\n",
    "dfTopCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __(d) Now, Calvin has watched the movie with the ID 110912. Print the top 3 movies based on the rating attribute, which have the same director as the watched movie with the ID 110912 as recommendations for him. The already watched movie should not be within the recommendations [0.5 points].__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetMovie = df[df['id'] == 110912]\n",
    "GetMovie\n",
    "## What are the rating attributes? just the rating or also the languages and genres? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSortedDirectorGenre = df[df['director'] == 'Quentin Tarantino']\n",
    "getSortedDirectorGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithoutPF = getSortedDirectorGenre.drop(29) #wollen nicht den Film den wir schon gesehen haben, darum entfernen\n",
    "dfWithoutPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinal = dfWithoutPF.sort_values(by=['rating'], ascending=False)\n",
    "dfFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTop3 = dfFinal.head(3)\n",
    "dfTop3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __(d) Now, Calvin has watched the movie with the ID 1502397 and we want to recommend movies with a similar plot. Remove all stopwords from the plot attribute and split the content of the plot attribute into terms. Then calculate the term similarity between the plot of the watched movie and all others. Use data pre-processing methods wherever needed. Print the top 3 movies based on the calculated term similarity and the rating of the movie.  [2 points].__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Movie = df[df['id'] == 1502397]\n",
    "Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re #Library um Zahlen zu entfernen\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "all_stopwords = stopwords.words('english') \n",
    "stopWord_list = [',', '(', ')', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '.']\n",
    "all_stopwords.extend(stopWord_list) #Stopwörter werden erweitert mit zusätzlichen Zeichen\n",
    "\n",
    "\n",
    "def getPlotWithoutStopwords(index):\n",
    "    selectedMovie = df[df['id'] == index]\n",
    "    Text = selectedMovie['plot'].to_string()\n",
    "    textWithoutNum = re.sub(r'\\d+', '', Text)\n",
    "    text_tokens = word_tokenize(textWithoutNum) #Wörter werden tokeniziert\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords] #Iteration um alle Stopwörter zu entfernen\n",
    "    return tokens_without_sw\n",
    "\n",
    "referenceTokens = getPlotWithoutStopwords(1502397)\n",
    "print(referenceTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distlist = []\n",
    "for idx in df.index:\n",
    "    tokens = getPlotWithoutStopwords(int(df.loc[idx, 'id']))\n",
    "    distlist.append(nltk.edit_distance(''.join(tokens), ''.join(referenceTokens)))\n",
    "df[\"LevenDistance\"] = distlist\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSorted = df.sort_values(by=['LevenDistance'], ascending=False)\n",
    "dfFinal = dfSorted.tail(3)\n",
    "dfFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Implement Collaborative Filtering Recommenders for the data in the ratings.csv file [6 points].\n",
    "##### __The ratings.csv file contains a dataset extracted from IMDB with ratings for movies from several users. Implement a user-based and an item-based collaborative filtering recommender. Use data pre-processing methods wherever needed. Print the top 3 recommended movies for each user in the dataset for the user-based and the item-based version. Don't recommend any movies the user has already watched.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df = pd.read_csv('ratings.csv')\n",
    "dm = pd.read_csv('movies.csv')\n",
    "pd.set_option('max_colwidth', None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    We first tried to generate the matrix of simulatiries manually but our code was really inefficient\n",
    "    calculation the entire matrix took way too long so we decided to generate the user_similarity matrix\n",
    "    using a pivot table and the sklearn library\n",
    "\n",
    "    https://stackoverflow.com/questions/45387476/cosine-similarity-between-each-row-in-a-dataframe-in-python\n",
    "    \n",
    "    print(generateManual())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#users = [\"not implemented\"]\n",
    "#\n",
    "#\n",
    "#def getUsers():\n",
    "#    users = df[\"user\"].drop_duplicates().tolist()\n",
    "#    return users\n",
    "#\n",
    "#\n",
    "#def getUserReview(user):\n",
    "#    # gahn dure und hole alli ratings wo user = user\n",
    "#    reviews = []\n",
    "#    for idx in df.index:\n",
    "#        if df.loc[idx, \"user\"] == user:\n",
    "#            reviews.append(df.loc[idx])\n",
    "#\n",
    "#    return pd.DataFrame(reviews, columns=[\"id\", \"user\", \"rating\"])\n",
    "#\n",
    "#\n",
    "#def getMovieReview(movieID):\n",
    "#    # gahn dure und hole alli ratings wo Movie = Movie\n",
    "#    reviews = []\n",
    "#    for idx in df.index:\n",
    "#        if df.loc[idx, \"id\"] == movieID:\n",
    "#            reviews.append(df.loc[idx])\n",
    "#    return pd.DataFrame(reviews, columns=[\"id\", \"user\", \"rating\"])\n",
    "#\n",
    "#\n",
    "#def findAB(userA, userB):\n",
    "#    sameReviewedMovies = []\n",
    "#    reviewlistA = getUserReview(userA)\n",
    "#    reviewlistB = getUserReview(userB)\n",
    "#    for idxOfA in reviewlistA.index:\n",
    "#        for idxOfB in reviewlistB.index:\n",
    "#            if reviewlistA.loc[idxOfA, \"id\"] == reviewlistB.loc[idxOfB, \"id\"]:\n",
    "#                sameReviewedMovies.append(reviewlistA.loc[idxOfA, \"id\"])\n",
    "#    return sameReviewedMovies\n",
    "#\n",
    "#\n",
    "#def similarity(userA, userB):\n",
    "#    same = findAB(userA, userB)\n",
    "#    ratingA = []\n",
    "#    # collet ratings of A\n",
    "#    for id in same:\n",
    "#        rating = df[(df[\"id\"] == id) & (df[\"user\"] == userA)]\n",
    "#        ratingA.append(rating[\"rating\"].tolist()[0])\n",
    "#\n",
    "#    ratingB = []\n",
    "#    # collet ratings of A\n",
    "#    for id in same:\n",
    "#        rating = df[(df[\"id\"] == id) & (df[\"user\"] == userB)]\n",
    "#        ratingB.append(rating[\"rating\"].tolist()[0])\n",
    "#\n",
    "#    upperTerm = 0\n",
    "#    for i in range(len(ratingA)):\n",
    "#        upperTerm += ratingA[i] * ratingB[i]\n",
    "#\n",
    "#    lowerTerm1 = 0\n",
    "#    allRatingsA = getUserReview(userA)[\"rating\"].tolist()\n",
    "#    for i in range(len(allRatingsA)):\n",
    "#        lowerTerm1 += allRatingsA[i] ** 2\n",
    "#\n",
    "#    lowerTerm2 = 0\n",
    "#    allRatingsB = getUserReview(userB)[\"rating\"].tolist()\n",
    "#    for i in range(len(allRatingsB)):\n",
    "#        lowerTerm2 += allRatingsB[i] ** 2\n",
    "#\n",
    "#    lowerTerm = np.sqrt(lowerTerm1 * lowerTerm2)\n",
    "#\n",
    "#    similarity = upperTerm / lowerTerm\n",
    "#\n",
    "#    return similarity\n",
    "#\n",
    "#\n",
    "#def applySimilarity(row):\n",
    "#    for idx in row.index:\n",
    "#        row.loc[idx] = similarity(row.name, idx)\n",
    "#    return row\n",
    "#\n",
    "#\n",
    "#def generateManual():\n",
    "#    ratings = df.pivot_table(index=\"user\", columns=\"id\", values=\"rating\", fill_value=0)\n",
    "#    user_similarity = pd.DataFrame(1, columns=ratings.index.values, index=ratings.index)\n",
    "#    user_similarity.apply(applySimilarity, axis=1)\n",
    "#    return user_similarity\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Exercise Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df[\"user\"].drop_duplicates().tolist()\n",
    "\n",
    "ratings = df.pivot_table(\n",
    "    index=\"user\", columns=\"id\", values=\"rating\", fill_value=0\n",
    ")  # Tabelle generieren mit User und Movie ID und deren Ratings\n",
    "\n",
    "\"\"\"\n",
    "In diesem Abschnitt werden die Ratings berechnet, dafür haben wir die vordefinierte Funktion cosine_similarity() verwendet von\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "Damit man cosinus_similarity verwenden kann, braucht man eine Matrix mit allen Ratings, die wir der Methode mitgeben.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "userSimilarityMatrix = pd.DataFrame(\n",
    "    cosine_similarity(ratings), columns=ratings.index.values, index=ratings.index\n",
    ")\n",
    "\n",
    "movies = dm.set_index(\"id\")  # Movies indexieren\n",
    "\n",
    "# Get all ratings of movies that the \"user\" did not saw\n",
    "def getUnseenRatings(ratings, user):\n",
    "    seen_movies = ratings.loc[user].replace(0, np.nan).dropna().index.values\n",
    "    all_movies = ratings.columns.values\n",
    "    unseen_movies = np.setdiff1d(all_movies, seen_movies)\n",
    "    return ratings.loc[getSimilarUsers(user).index, unseen_movies]\n",
    "\n",
    "\n",
    "# Multiply rating of unseen movies with simularity score of simlar users according to slide 23 of MLDM_05_RecommenderSystems_mony.pdf\n",
    "# Then devide the sum by\n",
    "def getRecommendationsOfRatings(ratings, user):\n",
    "    unseenRatings = getUnseenRatings(ratings, user)\n",
    "    sumOfRating = unseenRatings.mul(getSimilarUsers(user), axis=0).sum()\n",
    "    sumOfUserSimilarity = getSimilarUsers(user).sum()\n",
    "    return sumOfRating / sumOfUserSimilarity\n",
    "\n",
    "\n",
    "# Sort all users expcet the \"user\" according the the similary to the \"user\"\n",
    "def getSimilarUsers(user):\n",
    "    return userSimilarityMatrix.loc[user].sort_values(ascending=False).drop(user)\n",
    "\n",
    "\n",
    "def getUserBasedRecommendationsFor(user):\n",
    "    # Get all recommendations\n",
    "    recommendations = getRecommendationsOfRatings(ratings, user)\n",
    "    # Get the top 3 movies\n",
    "    recommendations = recommendations.sort_values(ascending=False).head(3)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "for user in users:\n",
    "    print(\n",
    "        \"The user-based recommendations for \"\n",
    "        + user\n",
    "        + \" are: \"\n",
    "        + str(getUserBasedRecommendationsFor(user))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df[\"user\"].drop_duplicates().tolist()\n",
    "\n",
    "ratings = df.pivot_table(\n",
    "    index=\"user\", columns=\"id\", values=\"rating\", fill_value=0\n",
    ")  # Tabelle generieren mit User und Movie ID und deren Ratings\n",
    "\n",
    "\"\"\"\n",
    "In diesem Abschnitt werden die Ratings berechnet, dafür haben wir die vordefinierte Funktion cosine_similarity() verwendet von\n",
    "from sklearn.metrics.paarweise import cosine_similarity\n",
    "Damit man cosinus_similarity verwenden kann, braucht man eine Matrix mit allen Ratings, die wir der Methode mitgeben. Da wir jetz\n",
    "die Movies vergleichen auf ähnlichkeit, Transponieren wir die Matrix Rating, um die Spalten auszurechnen\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "movieSimilarityMatrix = pd.DataFrame(\n",
    "    cosine_similarity(ratings.T), columns=ratings.T.index.values, index=ratings.T.index\n",
    ")\n",
    "\n",
    "movies = dm.set_index(\"id\")  # Movies indexieren\n",
    "# Get similar movies\n",
    "\n",
    "def getSimilarMovies(ratings, user):\n",
    "    seenMovies = ratings.loc[user].replace(0, np.nan).dropna().index.values\n",
    "    allMovies = ratings.columns.values\n",
    "    unseenMovies = np.setdiff1d(allMovies, seenMovies)\n",
    "    similarMovies = movieSimilarityMatrix[allMovies].drop(seenMovies, axis=0).drop(unseenMovies, axis=1)#vergleich mit der Folie wollen nur Filme empfehlen die der User nicht hat\n",
    "    return similarMovies\n",
    "\n",
    "\n",
    "# Multiply rating of unseen movies with simularity score of simlar users according to slide 27 of MLDM_05_RecommenderSystems_mony.pdf\n",
    "# Then devide the sum by\n",
    "def getRecommendationsOfRatings(ratings, user):\n",
    "    seenMovies = ratings.loc[user].replace(0, np.nan).dropna().index.values\n",
    "    similarMovies = getSimilarMovies(ratings, user)\n",
    "    sumOfRating = similarMovies.mul(ratings.T.loc[seenMovies, user]).sum(axis=1)\n",
    "    sumOfItemSimilarity = similarMovies.sum(axis=1)\n",
    "    return sumOfRating / sumOfItemSimilarity\n",
    "\n",
    "def getItemBasedRecommendationsFor(user):\n",
    "    # Get all recommendations\n",
    "    recommendations = getRecommendationsOfRatings(ratings, user)\n",
    "    # Get the top 3 movies\n",
    "    recommendations = recommendations.sort_values(ascending=False).head(3)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "for user in users:\n",
    "    print(\n",
    "        \"The Item-based recommendations for \"\n",
    "        + user\n",
    "        + \" are: \"\n",
    "        + str(getItemBasedRecommendationsFor(user))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Base Recommendation\n",
    "### ZHAW SW5 Item Based Recommendation with fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df = pd.read_csv('ratingsFolie.csv')\n",
    "dm = pd.read_csv('movies.csv')\n",
    "pd.set_option('max_colwidth', None)\n",
    "df\n",
    "\n",
    "users = df[\"user\"].drop_duplicates().tolist()\n",
    "\n",
    "ratings = df.pivot_table(\n",
    "    index=\"user\", columns=\"id\", values=\"rating\", fill_value=0\n",
    ") \n",
    "\n",
    "\n",
    "movieSimilarityMatrix = pd.DataFrame(\n",
    "    cosine_similarity(ratings.T), columns=ratings.T.index.values, index=ratings.T.index\n",
    ")\n",
    "\n",
    "movies = dm.set_index(\"id\")  \n",
    "\n",
    "def getSimilarMovies(ratings, user):\n",
    "    seenMovies = ratings.loc[user].replace(0, np.nan).dropna().index.values\n",
    "    allMovies = ratings.columns.values\n",
    "    unseenMovies = np.setdiff1d(allMovies, seenMovies)\n",
    "    similarMovies = movieSimilarityMatrix[allMovies].drop(seenMovies, axis=0).drop(unseenMovies, axis=1)\n",
    "    return similarMovies\n",
    "\n",
    "\n",
    "# Multiply rating of unseen movies with simularity score of simlar users according to slide 23 of MLDM_05_RecommenderSystems_mony.pdf\n",
    "# Then devide the sum by\n",
    "def getRecommendationsOfRatings(ratings, user):\n",
    "    seenMovies = ratings.loc[user].replace(0, np.nan).dropna().index.values\n",
    "    similarMovies = getSimilarMovies(ratings, user)\n",
    "    sumOfRating = similarMovies.mul(ratings.T.loc[seenMovies, user]).sum(axis=1)\n",
    "    sumOfItemSimilarity = similarMovies.sum(axis=1)\n",
    "    return sumOfRating / sumOfItemSimilarity\n",
    "\n",
    "def getItemBasedRecommendationsFor(user):\n",
    "    # Get all recommendations\n",
    "    recommendations = getRecommendationsOfRatings(ratings, user)\n",
    "    # Get the top 3 movies\n",
    "    recommendations = recommendations.sort_values(ascending=False).head(3)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "for user in users:\n",
    "    print(\n",
    "        \"The Item-based recommendations for \"\n",
    "        + user\n",
    "        + \" are: \"\n",
    "        + str(getItemBasedRecommendationsFor(user))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Extend the Collaborative Filtering Recommender [5 points].\n",
    "\n",
    "You are free to choose which additional task you want to do. Ideas could be:\n",
    "- Implement your own UI for the Recommendation System.\n",
    "- Create a blogpost on the internet describing your approach and the system (Note: Do not just upload the notebook but create a text with references to the literature and appropriate plots).\n",
    "- Test alternative approaches to create recommendations and compare them.\n",
    "- **... your own idea ...**\n",
    "\n",
    "If you solve this task in this notebook, just describe your idea in a few sentences. If you do not solve it in the notebook, please provide a more detailed description and screenshots if appropriate.\n",
    "\n",
    "This exercise must be presented during the lab session. The task is graded based on innovation, effort, result, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe your idea here:\n",
    "\n",
    "Our Idea is to make an analysis of our first attempt to solve the user based recommendations. Initially we tried to calculate the similarity matrix manually (without using prebuilt function from sklearn or pandas). When we tried to test our function we noticed that it took forever. We could not get any useful result. To find out whats wrong with our approach we are calculation the time complexity of our code\n",
    "Lets start by determining the time complexity for all our functions:\n",
    "\n",
    "n = number of reviews\n",
    "\n",
    "df = pd.read_csv('ratings.csv')\n",
    "dm = pd.read_csv('movies.csv')\n",
    "\n",
    "#### Time Complexity of pandas dataframe queries\n",
    "https://stackoverflow.com/questions/45240803/pandas-dataframe-search-is-linear-time-or-constant-time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### getUserReview()\n",
    "\n",
    "#### Description\n",
    "This function returns all the reviews that a specific user created.\n",
    "This function is used later to calculate the cosine similarity\n",
    "\n",
    "#### Complexity\n",
    "\n",
    "1 for loop iterating over all users = O(n)\n",
    "\n",
    "Total : = (n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserReview(user):\n",
    "    reviews = []\n",
    "    for idx in df.index:\n",
    "        if df.loc[idx, \"user\"] == user:\n",
    "            reviews.append(df.loc[idx])\n",
    "\n",
    "    return pd.DataFrame(reviews, columns=[\"id\", \"user\", \"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### findAB\n",
    "#### Description\n",
    "This function searches for movies that both userA and userB have reviewed.\n",
    "We need this function to calculate the upper fraction term of the cosine similarity formla\n",
    "#### Complexity\n",
    "\n",
    "\\+ getUserReview = O(n)\n",
    "\n",
    "\\+ getUserReview = O(n)\n",
    "\n",
    "\\+ 2 nested for loops over reviews = O(n^2)\n",
    "\n",
    "= \n",
    "\n",
    "O(n) + O(n) + O(n^2)\n",
    "\n",
    "Total :  O(n^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAB(userA, userB):\n",
    "    sameReviewedMovies = []\n",
    "    reviewlistA = getUserReview(userA)\n",
    "    reviewlistB = getUserReview(userB)\n",
    "    for idxOfA in reviewlistA.index:\n",
    "        for idxOfB in reviewlistB.index:\n",
    "            if reviewlistA.loc[idxOfA, \"id\"] == reviewlistB.loc[idxOfB, \"id\"]:\n",
    "                sameReviewedMovies.append(reviewlistA.loc[idxOfA, \"id\"])\n",
    "    return sameReviewedMovies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### similarity\n",
    "#### Description\n",
    "This function calculates the cosine similarity between two users according to the formula on the lectue slides. Here we make use of the functions defined above.\n",
    "#### Complexity\n",
    "findAB = O(n^2)\n",
    "\n",
    "\\+ for loop over ratings + query over dataframe = O(n^2)\n",
    "\n",
    "\\+ for loop over ratings + query over dataframe = O(n^2)\n",
    "\n",
    "\\+ for loop over ratings =  O(n)\n",
    "\n",
    "\\+ getUserReview = O(n)\n",
    "\n",
    "\\+ for loop over ratings =  O(n)\n",
    "\n",
    "\\+ getUserReview = O(n)\n",
    "\n",
    "\\+ for loop over ratings =  O(n)\n",
    "\n",
    "= O(n^2) + O(n^2) + O(n) + O(n) + O(n) + O(n)\n",
    "\n",
    "= 2* O(n^2) + 5*O(n)\n",
    "\n",
    "Total : O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(userA, userB):\n",
    "    same = findAB(userA, userB)\n",
    "    ratingA = []\n",
    "    # collet ratings of A\n",
    "    for id in same:\n",
    "        rating = df[(df[\"id\"] == id) & (df[\"user\"] == userA)]\n",
    "        ratingA.append(rating[\"rating\"].tolist()[0])\n",
    "\n",
    "    ratingB = []\n",
    "    # collet ratings of A\n",
    "    for id in same:\n",
    "        rating = df[(df[\"id\"] == id) & (df[\"user\"] == userB)]\n",
    "        ratingB.append(rating[\"rating\"].tolist()[0])\n",
    "\n",
    "    upperTerm = 0\n",
    "    for i in range(len(ratingA)):\n",
    "        upperTerm += ratingA[i] * ratingB[i]\n",
    "\n",
    "    lowerTerm1 = 0\n",
    "    allRatingsA = getUserReview(userA)[\"rating\"].tolist()\n",
    "    for i in range(len(allRatingsA)):\n",
    "        lowerTerm1 += allRatingsA[i] ** 2\n",
    "\n",
    "    lowerTerm2 = 0\n",
    "    allRatingsB = getUserReview(userB)[\"rating\"].tolist()\n",
    "    for i in range(len(allRatingsB)):\n",
    "        lowerTerm2 += allRatingsB[i] ** 2\n",
    "\n",
    "    lowerTerm = np.sqrt(lowerTerm1 * lowerTerm2)\n",
    "\n",
    "    similarity = upperTerm / lowerTerm\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### applySimilarity\n",
    "#### Description\n",
    "This function is just a small helper function to apply the cosine similarity to a row in a matrix. \n",
    "This is used in the next step\n",
    "#### Complexity\n",
    "\\+ similarity = O(n^2)\n",
    "\n",
    "\\+ for over cols in a row = num_of_cols * O(n^2)\n",
    "\n",
    "= (detailed calculation in next chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySimilarity(row):\n",
    "    for idx in row.index:\n",
    "        row.loc[idx] = similarity(row.name, idx)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### generateSimularityMatrix\n",
    "#### Description\n",
    "This function generates a pivot matrix of the users and then calculates the cosine similarity between them.\n",
    "As a first try we did this without any optimization for each collumn and row\n",
    "\n",
    "#### Complexity\n",
    "\\+ generating pivot table with m^2 entries\n",
    "\n",
    "\\+call applySimilarity for m rows where each row has m cols which results in :\n",
    "\n",
    "\\+O(n^2) * applySimilarity \n",
    "\n",
    "= O(n^2) * O(n^2) \n",
    "\n",
    "Total : O(n^4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSimularityMatrix():\n",
    "    ratings = df.pivot_table(index=\"user\", columns=\"id\", values=\"rating\", fill_value=0)\n",
    "    user_similarity = pd.DataFrame(1, columns=ratings.index.values, index=ratings.index)\n",
    "    user_similarity.apply(applySimilarity, axis=1)\n",
    "    return user_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Assuming one simple claculation including all the array access takes about 1msec. (This is just a guess, we did some testing : calculatin 1000 simple cosines takes about 1 msec).\n",
    "\n",
    "We got about about 80 unique users in our data. This means for our rough guess our function takes (80\\*80\\*80\\*80)\\*1msec = 41 000 000 msec = 12h\n",
    "Even though this is the maximum possible execution time we could not get any resulat with our first attemped. Thats why we dropped this attempt and used prebuilt function.\n",
    "\n",
    "To conclude this enitre thing: next time just use the prebuilt function for stuff like this since these are proballby heavily optimized already. Using the prebuilt functions the similarity matrix is built in an instant\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff3ff605b2dff129d4ac75cdfa4863ef8c3efe63dbcf5d0ddcaa539d0c68dadf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
